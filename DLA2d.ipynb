{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f61bba-0590-4051-a7b4-e10cb4a35298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c35209-774a-4865-a771-66e2efc86e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fa5c4-9ac6-40d3-8713-71578cc98b36",
   "metadata": {},
   "source": [
    "### Create DLA algorithm for 2D Matrix\n",
    "- Here point aggregator is simulated\n",
    "- additional funtion `DLA2D.trace_point()` is provided to trace the attachment of each point to graph\n",
    "**pseudocode**\n",
    "\n",
    "```\n",
    "dla{\n",
    "    initialize the matrix with needed parameter\n",
    "    mark the start point\n",
    "    generate a random point\n",
    "    while (point not attached to adjacent)\n",
    "        random walk around adjacent\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cfe76-0b43-441a-a287-bb8143789ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLA2d:\n",
    "    def __init__(self, matrix_size, stickiness, points_to_generate):\n",
    "        self.matrix_size = matrix_size\n",
    "        self.stickiness = stickiness\n",
    "        self.points_to_generate = points_to_generate\n",
    "    \n",
    "    def initialize_matrix(self):\n",
    "        matrix = np.zeros((self.matrix_size, self.matrix_size), dtype=np.int32)\n",
    "        # point aggregator\n",
    "        matrix[self.matrix_size//2, self.matrix_size//2] = 255\n",
    "        return matrix\n",
    "    \n",
    "    def get_adjacent_points(self, pos: tuple, padding: bool=True) -> list:\n",
    "        x, y = pos\n",
    "        adjacent_pos = [\n",
    "            (x - 1, y - 1),\n",
    "            (x - 1 if x-1 > -1 else self.matrix_size-1, y) if padding else (x - 1, y), #up\n",
    "            (x - 1, y + 1),\n",
    "            (x, y - 1 if y-1 > -1 else self.matrix_size-1) if padding else (x, y - 1), # left\n",
    "            (x, y + 1 if y+1 < self.matrix_size else 0) if padding else (x, y + 1), # right\n",
    "            (x + 1, y - 1),\n",
    "            (x + 1 if x+1 < self.matrix_size else 0, y) if padding else (x +1, y), # down\n",
    "            (x + 1, y + 1)]\n",
    "        adjacent_pos = filter(\n",
    "            lambda x: (x[0] > -1 and x[0] < self.matrix_size) and (x[1] > -1 and x[1] < self.matrix_size),\n",
    "            adjacent_pos)\n",
    "        return list(adjacent_pos)\n",
    "    \n",
    "    def get_random_pos(self, matrix: np.ndarray) -> tuple:\n",
    "        new_pos = (self.matrix_size//2, self.matrix_size//2)\n",
    "        while matrix[new_pos]==255:\n",
    "            new_pos = tuple(np.random.randint(0, self.matrix_size, size=2))\n",
    "        return new_pos, 1\n",
    "    \n",
    "    def start_random_walk(self, pos: tuple) -> tuple:\n",
    "        # S(t) = S(t-1) + w(t)\n",
    "        # w(t) is chosen from adjacent points with a probablistic inference from the distribution\n",
    "        # TODO: Optimize the walk\n",
    "        adj_pos = self.get_adjacent_points(pos)\n",
    "        prob = np.random.rand()\n",
    "        return adj_pos[np.random.randint(len(adj_pos))], prob if prob>0 else prob + 0.1\n",
    "    \n",
    "    def check_attachment(self, pos: tuple, prob: float, matrix: np.ndarray) -> bool:\n",
    "        adjacent = self.get_adjacent_points(pos, padding=False)\n",
    "        return any(filter(lambda x: matrix[x]==255, adjacent)) and prob < self.stickiness\n",
    "    \n",
    "    def generate_dla(self, matrix: np.ndarray):\n",
    "        self.path = []\n",
    "        for point in tqdm(range(self.points_to_generate)):\n",
    "            point_path = []\n",
    "            visit_matrix = np.zeros((self.matrix_size, self.matrix_size), dtype=np.int32)\n",
    "            new_pos, prob = self.get_random_pos(matrix)\n",
    "            visit_matrix[new_pos] = 1\n",
    "            point_path.append(new_pos)\n",
    "            while not self.check_attachment(new_pos, prob, matrix):\n",
    "                while visit_matrix[new_pos]!=0:\n",
    "                    new_pos, prob = self.start_random_walk(new_pos)\n",
    "                point_path.append(new_pos)\n",
    "                visit_matrix[new_pos] = 1\n",
    "            matrix[new_pos] = 255\n",
    "            self.path.append(point_path)\n",
    "        return matrix\n",
    "    \n",
    "    def trace_point(self, point: int):\n",
    "        data = np.empty((self.matrix_size, self.matrix_size), dtype=np.int32)\n",
    "        data.fill(255)\n",
    "        data[self.matrix_size//2, self.matrix_size//2] = np.random.randint(255)\n",
    "        if point < len(self.path) and point > 0:\n",
    "            for i in range(point):\n",
    "                data[self.path[i][-1]] = np.random.randint(255)\n",
    "            fig = plt.figure(figsize=(30, 30))\n",
    "            ax = plt.axes(xlim=(0, self.matrix_size), ylim=(0, self.matrix_size))\n",
    "\n",
    "            #plot the attached point/pixels till the recent point\n",
    "            ax.imshow(data.transpose(), interpolation='nearest', cmap='gray')\n",
    "\n",
    "            #mark the trace\n",
    "            trace = self.path[point-1]\n",
    "            x, y = trace[0]\n",
    "            x_, y_ = trace[-1]\n",
    "\n",
    "            #start marker\n",
    "            ax.scatter(x, y, marker='*', s=20*2*6, c='#17202A')\n",
    "\n",
    "            #end marker\n",
    "            ax.scatter(x_, y_, marker='*', s=20*4*6, c='#f20c27')\n",
    "\n",
    "            #trace curve\n",
    "            X, Y = zip(*trace)\n",
    "            line, = ax.plot(X, Y, lw=2, color='#0492C2')\n",
    "            ax.set_xticks(np.arange(0, self.matrix_size, 20))\n",
    "            ax.set_yticks(np.arange(0, self.matrix_size, 20))\n",
    "            ax.set_title('Random walk (Point Aggregator)', fontsize=22)\n",
    "            ax.set_xlabel('X-coordinate', fontsize=18)\n",
    "            ax.set_ylabel('Y-coordinate', fontsize=18)\n",
    "            ax.tick_params(labelsize=16)\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"Point number exceeds/underweighs points present in the matrix.\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25f767-29a9-48a4-9c64-d5dd65f4a355",
   "metadata": {},
   "source": [
    "## Task 1: Simulation of DLA for a given value of stickiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49c8ff-0ed5-42cf-b965-79e645558132",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dla = DLA2d(matrix_size=100, stickiness=1, points_to_generate=50)\n",
    "new_matrix = new_dla.initialize_matrix()\n",
    "mod_matrix = new_dla.generate_dla(matrix=copy.deepcopy(new_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e526b6-162d-489d-994d-ab9d4f181b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dla.trace_point(point=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb5a1b-34c3-48e9-a415-d3d04adf7808",
   "metadata": {},
   "source": [
    "## Task 2: An algorithm to estimate the \"stickiness\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd014632-4a16-4e34-ac90-00699361f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bd533-2fb8-4f71-a08e-0bfe3373cf58",
   "metadata": {},
   "source": [
    "### Create a helper module to generate dataset for estimating stickiness\n",
    "\n",
    "**pseudocode**\n",
    "\n",
    "```\n",
    "InferImage{\n",
    "    read the image/matrix n x n\n",
    "    take a filter m x m at center #As this is a point aggregator\n",
    "    Slice the matrix of shape m x m\n",
    "    calculate\n",
    "        surface area of m x m\n",
    "        crowd density of m x m\n",
    "        total point used on matrix n x n\n",
    "```\n",
    "- use `DLA2d` to create an image and the user `InferImage` to get the remaining datapoint\n",
    "- generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5a2f4-83c9-45f4-8c5f-9cdcfa60db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferImage:\n",
    "    def __init__(self, image:Union[np.ndarray, str], filter_size:int):\n",
    "        if (filter_size % 3) != 0:\n",
    "            filter_size = filter_size - 1\n",
    "        self.filter_size = filter_size\n",
    "        if isinstance(image, str):\n",
    "            img = Image.open(image)\n",
    "            img = img.convert(\"L\")\n",
    "            self.image = np.array(img)\n",
    "        else:\n",
    "            self.image = copy.deepcopy(image)\n",
    "        \n",
    "    def sliced_matrix(self):\n",
    "        if self.filter_size <= self.image.shape[0]:\n",
    "            middle_point = self.image.shape[0]//2\n",
    "            half_filter_size = self.filter_size//2\n",
    "            a = middle_point-half_filter_size\n",
    "            b = middle_point+half_filter_size + 1\n",
    "            return self.image[a:b, a:b]\n",
    "        raise ValueError(\"Reduce filter size\")\n",
    "        \n",
    "    def get_surface_area(self, matrix):\n",
    "        return np.count_nonzero(matrix)\n",
    "    \n",
    "    def get_crowd_density(self, matrix):\n",
    "        return self.get_surface_area(matrix)/(matrix.shape[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8290c-f462-4049-b983-e2eb788e8415",
   "metadata": {},
   "source": [
    "The datase contains the following information:\n",
    "\n",
    "**Given an image and filter size, all can be calculated.**\n",
    "\n",
    "| Image | filter size | surface area | Crowd density of points | Total points used | Stickiness |\n",
    "|-------|-------------|--------------|-------------------------|-------------------|------------|\n",
    "\n",
    "- Image: Point aggregator image generated from the above algorithm\n",
    "- filter size: filter matrix to crop image around center\n",
    "- surface area: surface area of pixel in the filter area\n",
    "- crowd density: crowd density of the points in the filter area\n",
    "- Total points used: Total points used for the generation of image\n",
    "- Stickiness: probablity with which a point is getting attached to another point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e91e0-cf67-461c-89ea-cf97d8bb1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as a pandas dataframe\n",
    "if not os.path.exists('dataset.pkl'):\n",
    "    print(\"File doesn't exists. Creating a new file.\")\n",
    "    dataset = pd.DataFrame(columns=[\n",
    "        'Image', 'filter size', 'surface area',\n",
    "        'Crowd density of points', 'Total points used', 'Stickiness'])\n",
    "    dataset.to_pickle(\"dataset.pkl\")\n",
    "else:\n",
    "    dataset = pd.read_pickle(\"dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3824d-ff59-48d6-9f8f-333ad2df3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the generated images at directory image_save_path\n",
    "image_save_path = './Image/'\n",
    "if not os.path.exists(image_save_path):\n",
    "    print(\"Directory not present. Creating a new one.\")\n",
    "    os.mkdir(image_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a03a9-7c91-474f-b5b2-9138d03223e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_runs = 0\n",
    "if not dataset.empty:\n",
    "    initial_runs = int(max(dataset['Image'].str.extract(pat=r'(\\d)', expand=False))) + 1\n",
    "    print(\"{} images have aleady been generated.\".format(initial_runs))\n",
    "runs = 1000 + initial_runs\n",
    "print(\"New images are to be generated with prefix starting from {} to {}\".format(initial_runs, runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783948a-fe97-49b8-8470-ae831d3aebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loop to generate the data and change the matrix size after frequency of 100\n",
    "\n",
    "matrix_size = 40\n",
    "filter_size = 9\n",
    "for run in range(initial_runs, runs):\n",
    "    k = np.random.uniform(0.01, 1)\n",
    "    dla = DLA2d(\n",
    "        matrix_size=matrix_size,\n",
    "        stickiness = k,\n",
    "        points_to_generate = np.random.randint(matrix_size, 100))\n",
    "    new_matrix = dla.initialize_matrix()\n",
    "    mod_matrix = dla.generate_dla(matrix=new_matrix)\n",
    "    file_name = image_save_path + \"run_{}.png\".format(run)\n",
    "    plt.imsave(file_name, mod_matrix)\n",
    "    \n",
    "    infer = InferImage(\n",
    "        image=mod_matrix,\n",
    "        filter_size = filter_size)\n",
    "    total_point = infer.get_surface_area(mod_matrix)\n",
    "    slc_matrix = infer.sliced_matrix()\n",
    "    surface_area = infer.get_surface_area(slc_matrix)\n",
    "    crowd_density = infer.get_crowd_density(slc_matrix)\n",
    "    \n",
    "    dataset = dataset.append(\n",
    "        {'Image': file_name,\n",
    "         'filter size': infer.filter_size,\n",
    "         'surface area': surface_area,\n",
    "         'Crowd density of points': crowd_density,\n",
    "         'Total points used': total_point,\n",
    "         'Stickiness': dla.stickiness\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "    dataset.to_pickle(\"dataset.pkl\")\n",
    "    if run % 100 == 0:\n",
    "        print(\"Run: {} completed.\".format(run))\n",
    "        matrix_size = matrix_size + 10\n",
    "        filter_size = filter_size + 2\n",
    "        print(\"New parameter\")\n",
    "        print(\"matrix({0}X{0}), filter({1}X{1})\".format(matrix_size, filter_size))\n",
    "    del dla, infer, new_matrix, mod_matrix, slc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37988022-1b46-4b91-9e98-8c61af823277",
   "metadata": {},
   "source": [
    "### Subtask: A basic regression model to check the hypothesis\n",
    "**Use a regression model to predict the stickiness in an image.**\n",
    "- Provided an image, surface area, Crowd density of points, Total points used can be calculated for a given filter size. Using them a value can be predicted as a stickiness parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37f9fd-24b8-427e-a8ea-d5b9ec2fd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"dataset.pkl\")\n",
    "dataset.set_index(\"Image\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca7a22-7df1-44eb-bb4d-94cba9bce960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d5edf-c3d9-44d9-b278-50bfe27d6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f815cbd-19c2-4814-b59f-20931ce2eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1292cd3-4f37-48c6-86e2-1a6418e23478",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66778c-9906-40ca-b2a6-569c6ee68438",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = svm.SVR(kernel='poly', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0df15-df63-4101-8521-cbf91cb869b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df1751-4331-43e7-94e5-3163ee2acd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 coefficient\n",
    "reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34b353-de3a-44b3-af39-7dc7bbbf8902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pipenv (extras)",
   "language": "python",
   "name": "extras-dwn1vi1x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
